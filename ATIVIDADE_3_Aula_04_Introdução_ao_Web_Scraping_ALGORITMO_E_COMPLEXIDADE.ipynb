{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1guel-RC/-Exercicio-Pr-tico-03---algorit-mo-e-complexidade/blob/main/ATIVIDADE_3_Aula_04_Introdu%C3%A7%C3%A3o_ao_Web_Scraping_ALGORITMO_E_COMPLEXIDADE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfIrFMqiWd_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ-Pd1JGWLsZ",
        "outputId": "86a8a7da-af02-4ded-e1af-e9c8c63345ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nenhuma notícia foi encontrada com a nova estratégia. O site pode ter mudado drasticamente.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def buscar_noticias_jc_uol():\n",
        "    \"\"\"\n",
        "    Realiza a raspagem de dados das manchetes principais do site JC UOL,\n",
        "    com limpeza de dados, exibe os resultados de forma organizada e salva em um arquivo CSV.\n",
        "    \"\"\"\n",
        "    url = 'https://jc.uol.com.br/'\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro ao acessar a página: {e}\")\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    lista_noticias = []\n",
        "    links_adicionados = set() # Usar um conjunto para controle rápido de duplicatas\n",
        "\n",
        "    # --- SELEÇÃO MELHORADA ---\n",
        "    # O site usa a classe 'c-headline' como um contêiner para cada notícia principal.\n",
        "    # Vamos iterar por esses contêineres para extrair os dados de forma confiável.\n",
        "    manchetes = soup.find_all('div', class_='c-headline')\n",
        "\n",
        "    for manchete in manchetes:\n",
        "        # Dentro de cada 'c-headline', o link e o título estão na tag <a>\n",
        "        link_tag = manchete.find('a', href=True)\n",
        "\n",
        "        if link_tag:\n",
        "            titulo = link_tag.get_text(strip=True)\n",
        "            link = link_tag['href']\n",
        "\n",
        "            # Verifica se o título não está vazio e se o link ainda não foi adicionado\n",
        "            if titulo and link not in links_adicionados:\n",
        "                dado = {\n",
        "                    \"titulo\": titulo,\n",
        "                    \"link\": link\n",
        "                }\n",
        "                lista_noticias.append(dado)\n",
        "                links_adicionados.add(link)\n",
        "\n",
        "    if not lista_noticias:\n",
        "        print(\"Nenhuma notícia foi encontrada com os seletores atuais.\")\n",
        "        return\n",
        "\n",
        "    # --- EXIBIÇÃO MELHORADA NO TERMINAL ---\n",
        "    print(\"--- MANCHETES ENCONTRADAS ---\")\n",
        "    for i, noticia in enumerate(lista_noticias, 1):\n",
        "        print(f\"\\nNotícia {i}:\")\n",
        "        print(f\"  Título: {noticia['titulo']}\")\n",
        "        print(f\"  Link: {noticia['link']}\")\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "    # --- SALVAR EM CSV ---\n",
        "    try:\n",
        "        nome_arquivo = 'noticias_jc_uol.csv'\n",
        "        df = pd.DataFrame(lista_noticias)\n",
        "        df.to_csv(nome_arquivo, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Dados salvos com sucesso no arquivo '{nome_arquivo}'!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao salvar o arquivo CSV: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    buscar_noticias_jc_uol()"
      ]
    }
  ]
}